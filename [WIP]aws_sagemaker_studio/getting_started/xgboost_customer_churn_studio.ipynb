{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon SageMaker Studio を一通り試す\n",
        "_**Gradient Boosted Trees を使って携帯電話の顧客離反を予測しよう**_\n",
        "\n",
        "---\n",
        "\n",
        "このノートブックでは、Amazon SageMaker の主な機能のうちいくつかを試すことができます。\n",
        "\n\n",
        "* [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html)\n",
        "  * 複数の機械学習の試行を管理する\n",
        "  * ハイパーパラメータに関する実験とグラフ化\n",
        "* [Amazon SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html)\n",
        "  * モデルのデバッグ\n",
        "* [Model hosting](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)\n",
        "  * モデルから予測を得るための永続的なエンドポイントをセットアップ\n",
        "* [SageMaker Model Monitor](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor.html)\n",
        "  * モデルの品質を監視する\n",
        "  * モデルの品質に変化があった場合にアラートをあげる\n",
        " \n",
        "\n",
        "---\n",
        "\n",
        "## コンテンツ\n",
        "\n",
        "1. [背景](#背景) - XGBoost による顧客離反予測について\n",
        "1. [データ](#データ) - データセットを用意して S3 にアップロード\n",
        "1. [学習](#学習) - Amazon SageMaker の XGBoost を利用して学習する\n",
        "  - [Amazon SageMaker Experiments](#Amazon-SageMaker-Experiments)\n",
        "  - [Amazon SageMaker Debugger](#Amazon-SageMaker-Debugger)\n",
        "1. [ホスティング](#ホスティング)\n",
        "1. [SageMaker Model Monitor](#SageMaker-Model-Monitor)\n",
        "\n",
        "---\n",
        "\n",
        "## 背景\n",
        "\n",
        "_このノートブックは [AWS blog post](https://aws.amazon.com/blogs/ai/predicting-customer-churn-with-amazon-machine-learning/)　を参考に作成しています。_.\n",
        "\n",
        "どのようなビジネスであっても、顧客を失うことは大きな損害です。もし、満足していない顧客を早期に見つけることができれば、そのような顧客をキープするためのインセンティブを提供できる可能性があるでしょう。このノートブックでは、満足していない顧客を自動で認識するために機械学習 (Machine Learning, ML) を利用する方法を説明します。そして、実験管理、モデル学習、デプロイ後のモデルの監視など、SageMaker の様々な機能を利用しています。\n",
        "\nまず、このノートブックを実行するために必要な Python ライブラリをインポートしましょう。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install sagemaker -U\n",
        "!{sys.executable} -m pip install sagemaker-experiments"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "from IPython.display import display\n",
        "from time import strftime, gmtime\n",
        "import boto3\n",
        "import re\n",
        "\n\n",
        "import sagemaker\n",
        "from sagemaker import get_execution_role\n",
        "from sagemaker.predictor import csv_serializer\n",
        "from sagemaker.debugger import rule_configs, Rule, DebuggerHookConfig\n",
        "from sagemaker.model_monitor import DataCaptureConfig, DatasetFormat, DefaultModelMonitor\n",
        "from sagemaker.s3 import S3Uploader, S3Downloader\n",
        "\n",
        "from smexperiments.experiment import Experiment\n",
        "from smexperiments.trial import Trial\n",
        "from smexperiments.trial_component import TrialComponent\n",
        "from smexperiments.tracker import Tracker"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sess = boto3.Session()\n",
        "sm = sess.client('sagemaker')\n",
        "role = sagemaker.get_execution_role()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## データ\n",
        "\n",
        "携帯電話会社は、どの顧客が最終的に離反したか、または、サービスを使い続けたかの履歴データをもっています。この履歴データに対して学習を行うことで、携帯電話会社の顧客離反を予想するモデルを構築します。モデルの学習が終わった後、任意の顧客のデータ (モデルの学習で利用したものと同じ情報を利用します）をモデルに入力すると、モデルはその顧客が離反しそうかどうかを予測します。\n",
        "\n",
        "ここで利用するデータセットは一般的に利用可能で、書籍 [Discovering Knowledge in Data](https://www.amazon.com/dp/0470908742/) の中で Daniel T. Larose が言及しているものです。そのデータセットは、著者によって University of California Irvine Repository of Machine Learning Datasets に提供されています。ここでは、そのデーセットをダウンロードして読み込んでみます。\n",
        "\n",
        "このノートブックと同じ階層にある `data` フォルダの中に、ダウンロードして処理済みのデータセットが保存されています。そして、学習セットとバリデーションセットに分割されています。データセットの前処理については、[XGBoost customer churn notebook that starts with the original dataset](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_applying_machine_learning/xgboost_customer_churn/xgboost_customer_churn.ipynb) をごらんください。\n",
        "\n",
        "学習はヘッダなしのcsvファイルを使って行いますが、以下のセルではヘッダ付きの学習データを`pandas`でロードしてみます。このデータセットがどういった特徴を持っていて、モデルの学習にどのようなデータが利用されるかを調べてみます。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path we can find the data files that go with this notebook\n",
        "%cd /root/amazon-sagemaker-examples/aws_sagemaker_studio/getting_started\n",
        "local_data_path = './data/training-dataset-with-header.csv'\n",
        "data = pd.read_csv(local_data_path)\n",
        "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
        "pd.set_option('display.max_rows', 10)         # Keep the output on one page\n",
        "data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで、学習用にS3にファイルをアップロードしますが、その前に、そのデータを保存するためのS3バケットがなければ作成しましょう。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "account_id = sess.client('sts', region_name=sess.region_name).get_caller_identity()[\"Account\"]\n",
        "bucket = 'sagemaker-studio-{}-{}'.format(sess.region_name, account_id)\n",
        "prefix = 'xgboost-churn'\n",
        "\n",
        "try:\n",
        "    if sess.region_name == \"us-east-1\":\n",
        "        sess.client('s3').create_bucket(Bucket=bucket)\n",
        "    else:\n",
        "        sess.client('s3').create_bucket(Bucket=bucket, \n",
        "                                        CreateBucketConfiguration={'LocationConstraint': sess.region_name})\n",
        "except Exception as e:\n",
        "    print(\"Looks like you already have a bucket of this name. That's good. Uploading the data files...\")\n",
        "\n",
        "# Return the URLs of the uploaded file, so they can be reviewed or used elsewhere\n",
        "s3url = S3Uploader.upload('data/train.csv', 's3://{}/{}/{}'.format(bucket, prefix,'train'))\n",
        "print(s3url)\n",
        "s3url = S3Uploader.upload('data/validation.csv', 's3://{}/{}/{}'.format(bucket, prefix,'validation'))\n",
        "print(s3url)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 学習\n",
        "\n学習に進みましょう! XGBoostというライブラリを使って、さきほどアップロードしたデータに対して、勾配ブースティング木 (Gradient Boosted Decision Trees)のモデルを学習します。XGBoostを使うにあたって、XGBoostアルゴリズムのコンテナの位置を最初に指定する必要があります。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
        "docker_image_name = get_image_uri(boto3.Session().region_name, 'xgboost', repo_version='0.90-2')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV のファイルを学習に利用するので、学習のための関数が、そのファイルがどこにあって、どういう形式なのかを`s3_input`を利用して指定します。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
        "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon SageMaker Experiments\n",
        "\n",
        "SageMakerの実験管理を使うことで、モデルの学習をトラックして、関連するモデルを構造的に管理し、モデルの構成、パラメータ、メトリクスのログを取り続けることができます。これによって、以前のモデルを再現したり、改善を加えたり、モデルの比較を行うことができます。ここでは、モデルを学習するための異なるアプローチについてトラックするために1つの実験を作成します。\n",
        "\n",
        "これから動かすアプローチや学習コードは1つの実験の試行となり、SageMaker Studioでは、これらの異なる試行を比較することができます。\n",
        "\n実験をここで作成してみましょう。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sess = sagemaker.session.Session()\n",
        "\n",
        "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
        "customer_churn_experiment = Experiment.create(experiment_name=\"customer-churn-prediction-xgboost-{}\".format(create_date), \n",
        "                                              description=\"Using xgboost to predict customer churn\", \n",
        "                                              sagemaker_boto_client=boto3.client('sagemaker'))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ハイパーパラメータ\n",
        "\n",
        "XGBoost に対してハイパーパラメータを設定することができます。重要なハイパーパラメータは以下のとおりです。\n",
        "\n",
        "- `max_depth` は、アルゴリズムがどこまで深い木を作成するかを決めます。より深い木はデータに対してより適合できますが、計算時間も長く過学習となる可能性があります。モデルの性能を改善するためには、浅い木をたくさん作成するか、深い木を少数作成するかというトレードオフに向き合う必要があります。\n",
        "- `subsample` は学習データのサンプリングをコントロールします。過学習を防ぐことができますが、あまり小さすぎる値を設定するとデータ不足に陥ります。\n",
        "- `num_round` はブースティングの回数をコントロールします。これまでの残差を使って学習されるモデルの数と本質的に同じです。多くの回数を実行すると学習により適しますが、計算時間画像化し、過学習に陥る可能性があります。\n",
        "- `eta` は各ブースティングがどの程度学習に反映されるかをコントロールします。より大きな値を設定すると、重みが小さくなり、保守的なブースティングになります。\n",
        "- `gamma` 木の大きさをコントロールします。より大きい値を設定すると保守的なブースティングになります。\n",
        "- `min_child_weight` も木の大きさをコントロールします。より大きい値を設定すると保守的なブースティングになります。\n",
        "\n詳細は [XGBoost's hyperparameters GitHub page](https://github.com/dmlc/xgboost/blob/master/doc/parameter.md)にて確認いただけます。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = {\"max_depth\":5,\n",
        "               \"subsample\":0.8,\n",
        "               \"num_round\":600,\n",
        "               \"eta\":0.2,\n",
        "               \"gamma\":4,\n",
        "               \"min_child_weight\":6,\n",
        "               \"silent\":0,\n",
        "               \"objective\":'binary:logistic'}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 試行 1 - アルゴリズムモードで XGBoost を利用する\n",
        "\n",
        "最初の試行では、ビルトインのXGBoostのコンテナを利用し、追加のコードを作成することなくモデルを学習します。この方法ではXGBoostを使ってモデルを学習し、他のビルトインアルゴリズムと同様にモデルのデプロイも可能です。\n",
        "\n",
        "このために`Trial`オブジェクトを作成し、そのTrialを先ほど作成した実験に関連付けます。モデルを学習するためにestimatorを作成し、使いたい学習インスタンスのタイプやその数、どこにモデルを保存するかといったパラメータを指定します。\n",
        "\n学習ジョブと、先ほど作成した実験の試行は `estimator.fit`を呼ぶときに関連付けられます。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "trial = Trial.create(trial_name=\"algorithm-mode-trial-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
        "                     experiment_name=customer_churn_experiment.experiment_name,\n",
        "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
        "\n",
        "xgb = sagemaker.estimator.Estimator(image_name=docker_image_name,\n",
        "                                    role=role,\n",
        "                                    hyperparameters=hyperparams,\n",
        "                                    train_instance_count=1, \n",
        "                                    train_instance_type='ml.m4.xlarge',\n",
        "                                    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
        "                                    base_job_name=\"demo-xgboost-customer-churn\",\n",
        "                                    sagemaker_session=sess)\n",
        "\n",
        "xgb.fit({'train': s3_input_train,\n",
        "         'validation': s3_input_validation}, \n",
        "        experiment_config={\n",
        "            \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
        "            \"TrialName\": trial.trial_name,\n",
        "            \"TrialComponentDisplayName\": \"Training\",\n",
        "        }\n",
        "       )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 結果を確認する\n",
        "\n",
        "学習ジョブが成功したら、SageMaker Studio の実験タブの中に、試行に関連するメトリクス、ログ、グラフを見ることができます。これらを見るために実験ボタンをクリックしてください。\n",
        "\n",
        "![experiments_button](./images/studio_experiments_icon.png)\n",
        "\n",
        "この実験のリストから、特定の実験をダブルクリックすると、その内容を見ることができます。複数の実験の内容を見たい場合は、controlをおしながらクリックしてそれらを選び、右クリックでコンテクストメニューを見ることができます。そこから、 \"Open in trial component list\" をクリックすることで、一緒に内容を見ることができます。これによって異なる実験を比較するようなグラフ化も可能です。\n",
        "\n",
        "![studio_experiments_list](./images/view_multiple_trial_components.gif)\n",
        "\n",
        "それぞれの内容は、最善のモデルが一番上に表示されるようにソートされています。\n",
        "\n",
        "![studio_trial_list](./images/studio_trial_list.png)\n",
        "\n",
        "#### モデルのダウンロード\n",
        "\n",
        "生成されたモデルを見つけてダウンロードすることができます。モデルを見つけるためには、左側のトレイの実験ボタンをクリックし、実験、最近の試行のリスト、それらの最近の内容をクリックしていくと、Describe Trial Componentsの画面が見えます。これには Artifacts のタブがあります。このタブでは\"Input Artifacts\"のところに学習データとバリデーションデータへのリンクが、そして \"Output Artifacts\"のところには生成されたモデルへのリンクが表示されています。\n",
        "\n![find the model artifact](./images/find_the_model_artifact.gif)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 他のハイパーパラメータを試す\n",
        "\n",
        "モデルを改善するためによく実施される方法としては、他のハイパーパラメータを試し、それらが最終的なバリデーションエラーに影響を与えるかどうかを見ることです。ここでは、`min_child_weight`のパラメータを変えて、それらの値を変えながら学習ジョブを実行し、バリデーションエラーが変わるかどうかを見ます。それぞれに対して、1つ1つ試行を作成することで、SageMaker Studioで結果を比較することができます。\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "min_child_weights = [1, 2, 4, 8, 10]\n",
        "\n",
        "for weight in min_child_weights:\n",
        "    hyperparams[\"min_child_weight\"] = weight\n",
        "    trial = Trial.create(trial_name=\"algorithm-mode-trial-{}-weight-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime()), weight), \n",
        "                         experiment_name=customer_churn_experiment.experiment_name,\n",
        "                         sagemaker_boto_client=boto3.client('sagemaker'))\n",
        "\n",
        "    t_xgb = sagemaker.estimator.Estimator(image_name=docker_image_name,\n",
        "                                          role=role,\n",
        "                                          hyperparameters=hyperparams,\n",
        "                                          train_instance_count=1, \n",
        "                                          train_instance_type='ml.m4.xlarge',\n",
        "                                          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
        "                                          base_job_name=\"demo-xgboost-customer-churn\",\n",
        "                                          sagemaker_session=sess)\n",
        "\n",
        "    t_xgb.fit({'train': s3_input_train,\n",
        "               'validation': s3_input_validation},\n",
        "                wait=False,\n",
        "                experiment_config={\n",
        "                    \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
        "                    \"TrialName\": trial.trial_name,\n",
        "                    \"TrialComponentDisplayName\": \"Training\",\n",
        "                }\n",
        "               )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### グラフの作り方\n",
        "\n",
        "試行の内容を複数選んでグラフを作ることができます。これは1回の学習を試しただけなので、わずかなデータしかなく、時系列でグラフ化するほどではないでしょう。しかし、パラメータの変化を散布図でプロットすることはできます。その例を以下の画像で示します。\n",
        "\n",
        "![scatter plot example](./images/scatter_plot_example.png)\n",
        "\n",
        "##### 散布図の作り方\n",
        "\n",
        "内容の複数選択を行い、\"Add chart\"をクリックします。Chart Properties のパネルで、データタイプとして\"Summary Statistics\"を選びます。グラフのタイプは scatter plot (散布図) を選びます。そして、`min_child_wight`をX軸に (これはノートブックで反復的に変化させた値です)、`validation:error_last`や `validation:error_avg` をY軸のメトリクスとして選択して、`trialComponentName`で色をつけます。\n",
        "\n",
        "![create a scatter plot](./images/create_a_scatter_plot.gif)\n",
        "\n",
        "選択する内容を変えることによって、その場でグラフを変更することも可能ですし、拡大縮小を行うこともできます。グラフ上に各項目を表示することで、各試行に対する情報を得ることができるでしょう。\n",
        "\n![adjust a scatter plot](./images/adjust_a_scatter_plot.gif)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amazon SageMaker Debugger\n",
        "\n",
        "Amazon SageMaker は学習時の機械学習モデルのデバッグを可能にします。学習の間、デバッガーは定期的にテンソルを保存し、そのテンソルはある瞬間における機械学習モデルの状態を完全に保持します。これらのテンソルは、SageMaker Studio を使って、学習時の課題を診断するための分析や可視化のためにS3に保存される。\n",
        "\n",
        "#### SageMaker のデバッグルールを指定する\n",
        "\n",
        "機械学習モデルの学習時に一般的に起こる課題を自動で検出するために、SageMaker debugger では、学習ジョブを評価する一連のリストを付与することを可能にしています。\n",
        "\n",
        "XGBoostに適用可能ないくつかのルール設定として、`AllZero`, `ClassImbalance`, `Confusion`, `LossNotDecreasing`, `Overfit`, `Overtraining`, `SimilarAcrossRuns`, `TensorVariance`, `UnchangedTensor`, `TreeDepth`があります。\n",
        "\nここでは、`LossNotDecreasing`という学習時のある時点でロスが低下しなくなることを検知するルールと、`Overtraining`のルール、`Overfit`のルールを適用します。これらのルールを作ってみましょう。"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "debug_rules = [Rule.sagemaker(rule_configs.loss_not_decreasing()),\n",
        "               Rule.sagemaker(rule_configs.overtraining()),\n",
        "               Rule.sagemaker(rule_configs.overfit())\n",
        "              ]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 試行2 - フレームワークモードでXGBoostを利用する\n",
        "\nFor the next trial, we'll train a similar model, but using XGBoost in framework mode. This way of using XGBoost should be to familiar to users who have worked with the open source XGBoost. Using XGBoost as a framework provides more flexibility than using it as a built-in algorithm as it enables more advanced scenarios that allow pre-processing and post-processing scripts to be incorporated into your training script. Specifically, we will be able to specify a list of rules that we want the SageMaker Debugger to evaluate our training process against."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Fit Estimator\n",
        "\n",
        "In order to use XGBoost as a framework you need to specify an entry-point script that can incorporate additional processing into your training jobs.\n",
        "\nWe have made a couple of simple changes to the  enable the SageMaker Debugger `smdebug`. Here we created a SessionHook which we pass as a callback function when creating a Booster. We passed a SaveConfig object telling the hook to save the evaluation metrics, feature importances, and SHAP values at regular intervals. Note that Sagemaker-Debugger is highly configurable, you can choose exactly what to save. The changes are described in a bit more detail below after we train this example as well as in even more detail in our [Developer Guide for XGBoost](https://github.com/awslabs/sagemaker-debugger/tree/master/docs/xgboost)."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "!pygmentize xgboost_customer_churn.py"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our Framwork estimator and call `fit` to start the training job. As before, we will create a separate trial for this run so that we can compare later using SageMaker Studio. Since we are running in framework mode we also need to pass additional parameters like the entry point script, and the framework version to the estimator. \n",
        "\nAs training progresses, you will be able to see logs from the SageMaker debugger evaluating the rule against the training job."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "entry_point_script = \"xgboost_customer_churn.py\"\n",
        "\n",
        "trial = Trial.create(trial_name=\"framework-mode-trial-{}\".format(strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())), \n",
        "                     experiment_name=customer_churn_experiment.experiment_name,\n",
        "                     sagemaker_boto_client=boto3.client('sagemaker'))\n",
        "\n",
        "framework_xgb = sagemaker.xgboost.XGBoost(image_name=docker_image_name,\n",
        "                                          entry_point=entry_point_script,\n",
        "                                          role=role,\n",
        "                                          framework_version=\"0.90-2\",\n",
        "                                          py_version=\"py3\",\n",
        "                                          hyperparameters=hyperparams,\n",
        "                                          train_instance_count=1, \n",
        "                                          train_instance_type='ml.m4.xlarge',\n",
        "                                          output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
        "                                          base_job_name=\"demo-xgboost-customer-churn\",\n",
        "                                          sagemaker_session=sess,\n",
        "                                          rules=debug_rules\n",
        "                                          )\n",
        "\n",
        "framework_xgb.fit({'train': s3_input_train,\n",
        "                   'validation': s3_input_validation}, \n",
        "                  experiment_config={\n",
        "                      \"ExperimentName\": customer_churn_experiment.experiment_name, \n",
        "                      \"TrialName\": trial.trial_name,\n",
        "                      \"TrialComponentDisplayName\": \"Training\",\n",
        "                  })"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the training has been running for a while you can view debug info in the Debugger panel. To get to this panel you must click through the experiment, trial, and then component.\n",
        "![view_debugger panel](./images/view_studio_debugger_output.gif)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Host\n",
        "\nNow that we've trained the model, let's deploy it to a hosted endpoint. In order to monitor the model once it is hosted and serving requests, we will also add configurations to capture data being sent to the endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "data_capture_prefix = '{}/datacapture'.format(prefix)\n",
        "\n",
        "endpoint_name = \"demo-xgboost-customer-churn-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
        "print(\"EndpointName = {}\".format(endpoint_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_predictor = xgb.deploy(initial_instance_count=1, \n",
        "                           instance_type='ml.m4.xlarge',\n",
        "                           endpoint_name=endpoint_name,\n",
        "                           data_capture_config=DataCaptureConfig(enable_capture=True,\n",
        "                                                                 sampling_percentage=100,\n",
        "                                                                 destination_s3_uri='s3://{}/{}'.format(bucket, data_capture_prefix)\n",
        "                                                                )\n",
        "                           )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Invoke the deployed model\n",
        "\nNow that we have a hosted endpoint running, we can make real-time predictions from our model very easily, simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_predictor.content_type = 'text/csv'\n",
        "xgb_predictor.serializer = csv_serializer\n",
        "xgb_predictor.deserializer = None"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll loop over our test dataset and collect predictions by invoking the XGBoost endpoint:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sending test traffic to the endpoint {}. \\nPlease wait for a minute...\".format(endpoint_name))\n",
        "\n",
        "with open('data/test_sample.csv', 'r') as f:\n",
        "    for row in f:\n",
        "        payload = row.rstrip('\\n')\n",
        "        response = xgb_predictor.predict(data=payload)\n",
        "        time.sleep(0.5)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Verify data capture in S3\n",
        "\n",
        "Since we made some real-time predictions by sending data to our endpoint we should have also captured that data for monitoring purposes. \n",
        "\n",
        "Let's list the data capture files stored in S3. You should expect to see different files from different time periods organized based on the hour in which the invocation occurred. The format of the s3 path is:\n",
        "\n`s3://{destination-bucket-prefix}/{endpoint-name}/{variant-name}/yyyy/mm/dd/hh/filename.jsonl`"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "current_endpoint_capture_prefix = '{}/{}'.format(data_capture_prefix, endpoint_name)\n",
        "print(\"Found Data Capture Files:\")\n",
        "capture_files = S3Downloader.list(\"s3://{}/{}\".format(bucket, current_endpoint_capture_prefix))\n",
        "print(capture_files)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the data captured is stored in a SageMaker specific json-line formatted file. Next, Let's take a quick peek at the contents of a single line in a pretty formatted json so that we can observe the format a little better."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "capture_file = S3Downloader.read_file(capture_files[-1])\n",
        "\n",
        "print(\"=====Single Data Capture====\")\n",
        "print(json.dumps(json.loads(capture_file.split('\\n')[0]), indent=2)[:2000])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, each inference request is captured in one line in the jsonl file. The line contains both the input and output merged together. In our example, we provided the ContentType as `text/csv` which is reflected in the `observedContentType` value. Also, we expose the enconding that we used to encode the input and output payloads in the capture format with the `encoding` value.\n",
        "\nTo recap, we have observed how you can enable capturing the input and/or output payloads to an Endpoint with a new parameter. We have also observed how the captured format looks like in S3. Let's continue to explore how SageMaker helps with monitoring the data collected in S3."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## SageMaker Model Monitor\n",
        "\n",
        "In addition to collecting the data, SageMaker provides capability for you to monitor and evaluate the data observed by the Endpoints. For this :\n",
        "1. We need to create a baseline with which we compare the realtime traffic against. \n",
        "1. Once a baseline is ready, we can set up a schedule to continously evaluate/compare against the baseline.\n",
        "1. We can send synthetic traffic to trigger alarms.\n",
        "\n",
        "**Important**: this section will take one hour or more to complete since the shortest monitoring polling time is one hour. See the following graphic to see how it looks after running for a few hours and some of the synthetic traffic triggered errors.\n",
        "\n![model monitor example](./images/view_model_monitor_output.gif)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baselining and continous monitoring\n",
        "\n",
        "#### 1. Constraint suggestion with baseline/training dataset\n",
        "\n",
        "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset data schema and the inference dataset schema should exactly match (ie number and type of the features).\n",
        "\nFrom our training dataset let's ask SageMaker to suggest a set of baseline `constraints` and generate descriptive `statistics` to explore the data. For this example, let's upload the training dataset which was used to train model. We will use the dataset file with column headers to have descriptive feature names."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_prefix = prefix + '/baselining'\n",
        "baseline_data_prefix = baseline_prefix + '/data'\n",
        "baseline_results_prefix = baseline_prefix + '/results'\n",
        "\n",
        "baseline_data_uri = 's3://{}/{}'.format(bucket,baseline_data_prefix)\n",
        "baseline_results_uri = 's3://{}/{}'.format(bucket, baseline_results_prefix)\n",
        "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
        "print('Baseline results uri: {}'.format(baseline_results_uri))\n",
        "baseline_data_path = S3Uploader.upload(\"data/training-dataset-with-header.csv\", baseline_data_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create a baselining job with training dataset\n",
        "\nNow that we have the training data ready in S3, let's kick off a job to `suggest` constraints. The convenient helper kicks off a `ProcessingJob` using a SageMaker provided ProcessingJob container to generate the constraints."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "my_default_monitor = DefaultModelMonitor(role=role,\n",
        "                                         instance_count=1,\n",
        "                                         instance_type='ml.m5.xlarge',\n",
        "                                         volume_size_in_gb=20,\n",
        "                                         max_runtime_in_seconds=3600,\n",
        "                                        )\n",
        "\n",
        "baseline_job = my_default_monitor.suggest_baseline(baseline_dataset=baseline_data_path,\n",
        "                                                   dataset_format=DatasetFormat.csv(header=True),\n",
        "                                                   output_s3_uri=baseline_results_uri,\n",
        "                                                   wait=True\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the job succeeds, we can explore the `baseline_results_uri` location in s3 to see what files where stored there."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Found Files:\")\n",
        "S3Downloader.list(\"s3://{}/{}\".format(bucket, baseline_results_prefix))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a`constraints.json` file that has information about suggested constraints. We also have a `statistics.json` which contains statistical information about the data in the baseline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "baseline_job = my_default_monitor.latest_baselining_job\n",
        "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
        "schema_df.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
        "constraints_df.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Analyzing Subsequent captures for data quality issues\n",
        "\nNow, that we have generated a baseline dataset and processed the baseline dataset to get baseline statistics and constraints, let's proceed monitor and analyze the data being sent to the endpoint with Monitoring Schedules."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Create a schedule\n",
        "Let's first create a Monitoring schedule for the previously created Endpoint. The schedule specifies the cadence at which we run a new processing job to compare recent data captures to the baseline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# First, copy over some test scripts to the S3 bucket so that they can be used for pre and post processing\n",
        "code_prefix = '{}/code'.format(prefix)\n",
        "pre_processor_script = S3Uploader.upload('preprocessor.py', 's3://{}/{}'.format(bucket,code_prefix))\n",
        "s3_code_postprocessor_uri = S3Uploader.upload('postprocessor.py', 's3://{}/{}'.format(bucket,code_prefix))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to create a model monitoring schedule for the Endpoint created before and also the baseline resources (constraints and statistics) which were generated above."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.model_monitor import CronExpressionGenerator\n",
        "from time import gmtime, strftime\n",
        "\n",
        "reports_prefix = '{}/reports'.format(prefix)\n",
        "s3_report_path = 's3://{}/{}'.format(bucket,reports_prefix)\n",
        "\n",
        "mon_schedule_name = 'demo-xgboost-customer-churn-model-schedule-' + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
        "my_default_monitor.create_monitoring_schedule(monitor_schedule_name=mon_schedule_name,\n",
        "                                              endpoint_input=xgb_predictor.endpoint,\n",
        "                                              #record_preprocessor_script=pre_processor_script,\n",
        "                                              post_analytics_processor_script=s3_code_postprocessor_uri,\n",
        "                                              output_s3_uri=s3_report_path,\n",
        "                                              statistics=my_default_monitor.baseline_statistics(),\n",
        "                                              constraints=my_default_monitor.suggested_constraints(),\n",
        "                                              schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
        "                                              enable_cloudwatch_metrics=True,\n",
        "                                             )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Start generating some artificial traffic\n",
        "The block below kicks off a thread to send some traffic to the created endpoint. This is so that we can continue to send traffic to the endpoint so that we will have always have data continually captured for analysis. If there is no traffic, the monitoring jobs will start to fail later on.\n",
        "\nNote that you need to stop the kernel to terminate this thread."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from threading import Thread\n",
        "from time import sleep\n",
        "import time\n",
        "\n",
        "runtime_client = boto3.client('runtime.sagemaker')\n",
        "\n",
        "# (just repeating code from above for convenience/ able to run this section independently)\n",
        "def invoke_endpoint(ep_name, file_name, runtime_client):\n",
        "    with open(file_name, 'r') as f:\n",
        "        for row in f:\n",
        "            payload = row.rstrip('\\n')\n",
        "            response = runtime_client.invoke_endpoint(EndpointName=ep_name,\n",
        "                                          ContentType='text/csv', \n",
        "                                          Body=payload)\n",
        "            time.sleep(1)\n",
        "            \n",
        "def invoke_endpoint_forever():\n",
        "    while True:\n",
        "        invoke_endpoint(endpoint_name, 'data/test-dataset-input-cols.csv', runtime_client)\n",
        "        \n",
        "thread = Thread(target = invoke_endpoint_forever)\n",
        "thread.start()\n",
        "\n# Note that you need to stop the kernel to stop the invocations"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### List executions\n",
        "Once the schedule is scheduled, it will kick of jobs at specified intervals. Here we are listing the latest 5 executions. Note that if you are kicking this off after creating the hourly schedule, you might find the executions empty. You might have to wait till you cross the hour boundary (in UTC) to see executions kick off. The code below has the logic for waiting."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "mon_executions = my_default_monitor.list_executions()\n",
        "if len(mon_executions) == 0:\n",
        "    print(\"We created a hourly schedule above and it will kick off executions ON the hour.\\nWe will have to wait till we hit the hour...\")\n",
        "\n",
        "while len(mon_executions) == 0:\n",
        "    print(\"Waiting for the 1st execution to happen...\")\n",
        "    time.sleep(60)\n",
        "    mon_executions = my_default_monitor.list_executions()  "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Evaluate the latest execution and list the generated reports"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "latest_execution = mon_executions[-1]\n",
        "print(\"Latest execution result: {}\".format(latest_execution.describe()['ExitMessage']))\n",
        "report_uri = latest_execution.output.destination\n",
        "\n",
        "print(\"Found Report Files:\")\n",
        "S3Downloader.list(report_uri)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### List violations\n",
        "\nIf there are any violations compared to the baseline, it will be generated here. Let's list the violations."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "violations = my_default_monitor.latest_monitoring_constraint_violations()\n",
        "pd.set_option('display.max_colwidth', -1)\n",
        "constraints_df = pd.io.json.json_normalize(violations.body_dict[\"violations\"])\n",
        "constraints_df.head(10)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can plug in the processing job arn for a single execution of the monitoring into this notebook to see more detailed visualizations of the violations and distribution statistics of the data captue that was processed in that execution\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean-up\n",
        "\n",
        "If you're ready to be done with this notebook, please run the cell below.  This will remove the hosted endpoint you created and avoid any charges from a stray instance being left on. It will also clean up all artifacts related to the experiments. \n",
        "\nYou may also want to delete artifacts stored in the associated s3 bucket used in this notebook by going to the S3 console and finding the bucket `sagemaker-studio-<region-name>-<account-name>` and deleting the files associated with this notebook."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sess.delete_monitoring_schedule(mon_schedule_name)\n",
        "sess.delete_endpoint(xgb_predictor.endpoint)\n",
        "def cleanup(experiment):\n",
        "    '''Clean up everything in the given experiment object'''\n",
        "    for trial_summary in experiment.list_trials():\n",
        "        trial = Trial.load(trial_name=trial_summary.trial_name)\n",
        "        \n",
        "        for trial_comp_summary in trial.list_trial_components():\n",
        "            trial_step=TrialComponent.load(trial_component_name=trial_comp_summary.trial_component_name)\n",
        "            print('Starting to delete TrialComponent..' + trial_step.trial_component_name)\n",
        "            sm.disassociate_trial_component(TrialComponentName=trial_step.trial_component_name, TrialName=trial.trial_name)\n",
        "            trial_step.delete()\n",
        "            time.sleep(1)\n",
        "         \n",
        "        trial.delete()\n",
        "    \n",
        "    experiment.delete()\n",
        "\ncleanup(customer_churn_experiment)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "Python 3 (Data Science)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:environment/datascience"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}