{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker ノートブックインスタンスで AWS Glue と Amazon Athena を用いたデータの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本ハンズオンでは Amazon SageMaker で機械学習モデルを学習させるためのデータを、Amazon S3、Amazon Athena、AWS Glue をSageMakerと連携させて前処理を行う場合についてご体験頂きます。\n",
    "\n",
    "---\n",
    "### 実施内容\n",
    "\n",
    "1. [データの準備](#1.データの準備)\n",
    "1. [Athenaを使ったデータの確認](#2.Athenaを使ったデータの確認)\n",
    "1. [Glueによるデータの前処理の実施](#3.Glueによるデータの前処理の実施)\n",
    "1. [モデルの学習](#4.モデルの学習)\n",
    "1. [後片付け](#5.後片付け)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "import pandas\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.データの準備\n",
    "\n",
    "今回は [TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) にある[2019年6月の Green Taxi Trip Records(CSV)](https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2019-06.csv) を利用します。このデータを今回のユースケースである、ノートブックでの展開が困難であるほど容量が大きいデータだと仮定し、一度 Amazon S3 へアップロードした上で、ノートブックインスタンスから削除します。作成後、[マネジメントコンソール](https://s3.console.aws.amazon.com/s3/home)で確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのダウンロード\n",
    "!wget https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2019-06.csv\n",
    "\n",
    "# 本ハンズオンで活用するバケットの作成\n",
    "bucket_name='data-wrangler-{0:%Y%m%d-%H%M%S}'.format(datetime.now())\n",
    "s3 = boto3.resource('s3')\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.create()\n",
    "\n",
    "# バケットへのデータのアップロード\n",
    "s3.Object(bucket_name, 'green_tripdata_2019-06.csv').upload_file('green_tripdata_2019-06.csv')\n",
    "print('s3://{} へデータがアップロードされました。'.format(bucket_name))\n",
    "\n",
    "# ノートブックインスタンス上でのデータの削除\n",
    "!rm green_tripdata_2019-06.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Athenaを使ったデータの確認\n",
    "### Amazon Athena のセットアップ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena のデータベースを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ath = boto3.client('athena')\n",
    "database_name='datawrangler'\n",
    "\n",
    "# データベースの作成\n",
    "ath.start_query_execution(\n",
    "    QueryString='CREATE DATABASE {}'.format(database_name),\n",
    "    ResultConfiguration={'OutputLocation': 's3://' + bucket_name + '/athena/'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したデータベースにテーブルを作成します。 [マネジメントコンソール](https://console.aws.amazon.com/athena/home) で確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = \"\"\"\n",
    "CREATE EXTERNAL TABLE green_tripdata(\n",
    "  VendorID string, \n",
    "  lpep_pickup_datetime string,\n",
    "  lpep_dropoff_datetime string,\n",
    "  store_and_fwd_flag string,\n",
    "  RatecodeID string,\n",
    "  PULocationID string,\n",
    "  DOLocationID string,\n",
    "  passenger_count int,\n",
    "  trip_distance double,\n",
    "  fare_amount double,\n",
    "  extra double,\n",
    "  mta_max double,\n",
    "  tip_amount double,\n",
    "  tolls_amount double,\n",
    "  ehail_fee string,\n",
    "  improvement_surcharge double,\n",
    "  total_amount double,\n",
    "  payment_type string,\n",
    "  trip_type string,\n",
    "  congestion_surcharge double\n",
    "  )\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' \n",
    "LOCATION 's3://{}/';\n",
    "\"\"\".format(bucket_name)\n",
    "\n",
    "ath.start_query_execution(\n",
    "        QueryString=query_string,\n",
    "        QueryExecutionContext={'Database': database_name},\n",
    "        ResultConfiguration={'OutputLocation': 's3://' + bucket_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Wrangler と Athena を使ったデータ分析\n",
    "2019年9月、 [Github](https://github.com/awslabs/aws-data-wrangler) に AWS Data Wrangler が公開されました。Data Wranglerは、各種AWSサービスからデータを取得して、コーディングをサポートしてくれるPythonのモジュールです。 PyAthena や boto3、を活用する場合に比べて、接続設定などのコーディングが簡素になるため、より一層データ分析や ETL 処理に集中することが出来ます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Wrangler をインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athea で使うクエリをこのノートブックインスタンス上から実行してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler\n",
    "\n",
    "# データを確認するためのSQL文\n",
    "sql = \"\"\"\n",
    "SELECT \n",
    "    * \n",
    "FROM\n",
    "    green_tripdata\n",
    "ORDER BY \n",
    "    RAND()\n",
    "LIMIT\n",
    "    1000\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "session = awswrangler.Session()\n",
    "df = session.pandas.read_sql_athena(sql=sql,database=database_name)\n",
    "\n",
    "# 取得したデータの表示\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 にあるデータを Athena やサンプリングされたデータを用いてノートブックインスタンス上の [Pandas](https://pandas.pydata.org/) を使って探索的データ分析を行い、機械学習モデルの学習に必要な前処理や特徴量作成を検討します。その後、実際の処理は Glue 上で実施します。今回のデータの前処理を行うスクリプトを確認しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Glueによるデータの前処理の実施"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat preprocess.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回準備したスクリプトを S3 へアップロードします。Glue のジョブ作成時にそのスクリプトのパスを指定することで実行が可能です。ジョブ作成時の`Role`を指定して下さい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Glue で使用するRoleの指定\n",
    "role = '使用するRoleを指定して下さい'\n",
    "\n",
    "# バケットへのデータのアップロード\n",
    "s3.Object(bucket_name, 'preprocess.py').upload_file('preprocess.py')\n",
    "print('s3://{} へ Glue のスクリプトがアップロードされました。'.format(bucket_name))\n",
    "\n",
    "# Glue のジョブを作成します。\n",
    "glue = boto3.client('glue')\n",
    "job = glue.create_job(Name='preprocess', Role=role,\n",
    "                      Command={'Name': 'pythonshell',\n",
    "                               'ScriptLocation': 's3://{}/preprocess.py'.format(bucket_name),\n",
    "                               'PythonVersion': '3'})\n",
    "# 作成したジョブを開始します。\n",
    "jobrun = glue.start_job_run(JobName = job['Name'],Arguments = {'--bucket_name': bucket_name} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ジョブの実行状況を確認しましょう。`SUCCEEDED` となったら完了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "while True:\n",
    "    status = glue.get_job_run(JobName=job['Name'], RunId=jobrun['JobRunId'])\n",
    "    print('Glue のジョブのステータスは現在 {} です。'.format(status['JobRun']['JobRunState']))\n",
    "    if status['JobRun']['JobRunState'] == 'SUCCEEDED':\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.モデルの学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glue による処理により、S3 のバケット上に `train.csv` と `validation.csv` が作成されています。今回はこのデータをモデルの学習に使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのパスを指定\n",
    "input_train = 's3://{}/train.csv'.format(bucket_name)\n",
    "input_validation = 's3://{}/validation.csv'.format(bucket_name)\n",
    "\n",
    "s3_input_train = s3_input(s3_data=input_train, content_type='text/csv')\n",
    "s3_input_validation = s3_input(s3_data=input_validation, content_type='text/csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは学習を始めましょう。まず、XGBoost のコンテナの場所を取得します。コンテナ自体は SageMaker 側で用意されているので、場所を指定すれば利用可能です。XGBoostのhyperparameterに関する詳細は [github](https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst) もチェックしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='reg:linear',\n",
    "                        num_round=100)\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`job completed`という文字が出たら学習の完了です。このモデルを使って推論の実行を行うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.後片付け"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本ハンズオンで使用したリソースを削除します。今回使ったリソースは下記になります。\n",
    "- Athena のテーブル\n",
    "- Glue のジョブ\n",
    "- Glue のデータベース\n",
    "- S3 のバケット\n",
    "\n",
    "SageMakerへ割り当てられているロールによっては削除の権限がない場合があります。その場合には[マネジメントコンソール](https://console.aws.amazon.com/console/home?)から削除して下さい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Athena のテーブルを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = 'DROP TABLE `green_tripdata`;'\n",
    "\n",
    "ath.start_query_execution(\n",
    "        QueryString=query_string,\n",
    "        QueryExecutionContext={'Database': database_name},\n",
    "        ResultConfiguration={'OutputLocation': 's3://' + bucket_name})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glue のジョブとデータベースを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glue.delete_job(JobName='preprocess')\n",
    "glue.delete_database(Name=database_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S3 のバケットを削除します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本ハンズオンで活用したバケットの削除\n",
    "bucket = s3.Bucket(bucket_name)\n",
    "bucket.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
