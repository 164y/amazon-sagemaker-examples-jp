{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow BYOM: 独自の学習スクリプトを用いたモデルの学習と、デプロイ手段の比較\n",
    "\n",
    "本ハンズオンでは [TensorFlow MNIST distributed training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/sagemaker-python-sdk/tensorflow_distributed_mnist/tensorflow_distributed_mnist.ipynb) と同様に [MNIST dataset](http://yann.lecun.com/exdb/mnist/) を TensorFlow で分散学習を行います。その後、CPU () を用いた推論インスタンスへのデプロイ、Elastic Inference を用いた推論インスタンスへのデプロイ、SageMaker Neo を用いてモデルをコンパイルした上での推論インスタンスへのデプロイを行います。\n",
    "\n",
    "---\n",
    "\n",
    "## コンテンツ\n",
    "\n",
    "1. [環境のセットアップ](#1.環境のセットアップ)\n",
    "1. [学習データの準備](#2.学習データの準備)\n",
    "1. [分散学習用のスクリプトの準備](#3.分散学習用のスクリプトの準備)\n",
    "1. [TensorFlow Estimator を利用して学習ジョブを作成する](#4.TensorFlowEstimatorを利用して学習ジョブを作成する)\n",
    "1. [学習したモデルをエンドポイントにデプロイする](#5.学習したモデルをエンドポイントにデプロイする)\n",
    "1. [エンドポイントを呼び出し推論を実行する](#6.エンドポイントを呼び出し推論を実行する)\n",
    "1. [エンドポイントを削除する](#7.エンドポイントを削除する)\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境のセットアップ\n",
    "\n",
    "まずは環境のセットアップを行いましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import time\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.学習データの準備\n",
    "MNIST データセットのダウンロードし。学習、評価、テスト用のそれぞれのデータへ分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from tensorflow.contrib.learn.python.learn.datasets import mnist\n",
    "import tensorflow as tf\n",
    "\n",
    "data_sets = mnist.read_data_sets('data', dtype=tf.uint8, reshape=False, validation_size=5000)\n",
    "\n",
    "utils.convert_to(data_sets.train, 'train', 'data')\n",
    "utils.convert_to(data_sets.validation, 'validation', 'data')\n",
    "utils.convert_to(data_sets.test, 'test', 'data')\n",
    "\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを Amazon S3 上へアップロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-mnist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 分散学習での学習スクリプトの準備\n",
    "\n",
    "このチュートリアルのトレーニングスクリプトは、TensorFlowの公式の[CNN MNISTの例](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/layers/cnn_mnist.py) をベースに作成されました。 SageMaker から渡された `` model_dir`` パラメーターを処理するように変更しています。 これは、分散学習時のデータ共有、チェックポイント、モデルの永続保存などに使用できるS3パスです。 また、トレーニング関連の変数を扱うために、引数をパースする関数も追加しました。\n",
    "\n",
    "スクリプト全体は次のとおりです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat 'mnist.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.TensofFlow Estimator による学習ジョブの作成\n",
    "\n",
    "`sagemaker.tensorflow.TensorFlow`　estimator は、TensorFlow コンテナの指定、学習・推論スクリプトの S3 へのアップロード、および SageMaker トレーニングジョブの作成を行います。ここでいくつかの重要なパラメーターを呼び出しましょう。\n",
    "\n",
    "`distributions` は、分散トレーニング設定を構成するために使用されます。インスタンスのクラスターまたは複数の GPU をまたいで分散学習を行う場合にのみ必要です。ここでは、分散トレーニングスキーマとしてパラメーターサーバーを使用しています。 SageMaker トレーニングジョブは同種のクラスターで実行されます。 SageMaker セットアップでパラメーターサーバーのパフォーマンスを向上させるために、クラスター内のすべてのインスタンスでパラメーターサーバーを実行するため、起動するパラメーターサーバーの数を指定する必要はありません。スクリプトモードは、[Horovod](https://github.com/horovod/horovod) による分散トレーニングもサポートしています。 `distributions` の設定方法に関する詳細なドキュメントは[こちら](https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/tensorflow#distributed-training) をご参照ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "mnist_estimator = TensorFlow(entry_point='mnist.py',\n",
    "                             role=role,\n",
    "                             framework_version='1.11.0',\n",
    "                             training_steps=1000, \n",
    "                             evaluation_steps=100,\n",
    "                             train_instance_count=2,\n",
    "                             train_instance_type='ml.p2.xlarge')\n",
    "\n",
    "mnist_estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ``fit`` による学習ジョブの実行\n",
    "\n",
    "学習ジョブを開始するには、`estimator.fit（training_data_uri）` を呼び出します。\n",
    "\n",
    "ここでは、S3 ロケーションが入力として使用されます。 `fit` は、`training` という名前のデフォルトチャネルを作成します。これは、このS3ロケーションを指します。トレーニングスクリプトでは、 `SM_CHANNEL_TRAINING` に保存されている場所からトレーニングデータにアクセスできます。 `fit`は、他のいくつかのタイプの入力も受け入れます。詳細については、APIドキュメント[こちら](https://sagemaker.readthedocs.io/en/stable/estimators.html#sagemaker.estimator.EstimatorBase.fit) を参照してください。\n",
    "\n",
    "トレーニングが開始されると、TensorFlow コンテナは mnist.py を実行し、スクリプトの引数として　estimator から`hyperparameters` と `model_dir` を渡します。この例では、estimator 内で定義していないハイパーパラメーターは渡されず、 `model_dir` のデフォルトは `s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>` であるため、スクリプトの実行は次のようになります。\n",
    "```bash\n",
    "python mnist.py --model_dir s3://<DEFAULT_BUCKET>/<TRAINING_JOB_NAME>\n",
    "```\n",
    "トレーニングが完了すると、トレーニングジョブは保存されたモデルを TensorFlow serving にアップロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CPUインスタンスを用いた推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 m4.xlarge CPU インスタンスのデプロイ\n",
    "`deploy（）`メソッドは SageMaker モデルを作成します。このモデルはエンドポイントにデプロイされ、リアルタイムで予測リクエストを処理します。まずは`ml.m4.xlarge` インスタンスを用いた推論インスタンスをデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_cpu = mnist_estimator.deploy(initial_instance_count=1,\n",
    "                                     instance_type='ml.m4.xlarge',\n",
    "                                     wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 m4.xlarge CPU インスタンスでのリアルタイム推論\n",
    "モデルをデプロイしたエンドポイントに対して、データを入力し、推論結果を得ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_times = []\n",
    "for i in range(20):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    \n",
    "    start = time.time()\n",
    "    predict_response = predictor_cpu.predict(data)\n",
    "    process_time = time.time() - start\n",
    "    process_times.append(process_time)\n",
    "    \n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    prediction = predict_response['outputs']['classes']['int64_val'][0]\n",
    "    print(\"label is {}, prediction is {}\".format(label, prediction))\n",
    "    \n",
    "print(\"推論実行時間の平均は {} です。\".format(np.array(process_times).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Elastic Inference を使った推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Elastic Inference を用いた推論インスタンスへのデプロイ\n",
    "Amazon Elastic Inference (EI) を使用することで、Amazon SageMaker がホストするモデルとしてデプロイされている深層学習モデルからのスループットを高速化し、リアルタイムの推論を得るためのレイテンシーを短縮することができます。さらに、エンドポイントに GPU インスタンスを使用するコストと比較して、大幅なコストダウンを実現できます。`accelerator_type` を指定することで Elastic Inference を使った推論インスタンスをデプロイできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "saved_model = mnist_estimator.model_data\n",
    "\n",
    "# モデルの作成\n",
    "estimator_ei = Model(model_data=saved_model,\n",
    "                     role=role,\n",
    "                     framework_version='1.11')\n",
    "\n",
    "# モデルのデプロイ\n",
    "predictor_ei = estimator_ei.deploy(initial_instance_count=1,\n",
    "                                   instance_type='ml.m4.xlarge',\n",
    "                                   accelerator_type='ml.eia1.xlarge',\n",
    "                                   wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Elastic Inference を使ったリアルタイム推論\n",
    "同様に、推論結果を得ます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_times = []\n",
    "\n",
    "for i in range(20):\n",
    "    data = mnist.test.images[i].tolist()\n",
    "    \n",
    "    start = time.time()\n",
    "    predict_response = predictor_ei.predict(data)\n",
    "    process_time = time.time() - start\n",
    "    process_times.append(process_time)\n",
    "    \n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    prediction = predict_response['predictions'][0]['classes']\n",
    "    print(\"label is {}, prediction is {}\".format(label, prediction))\n",
    "    \n",
    "print(\"推論実行時間の平均は {} です。\".format(np.array(process_times).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. SageMaker Neo を使った推論"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 SageMaker Neo でのモデルのコンパイル\n",
    "\n",
    "SageMaker Neo API を用いれば学習済みのモデルを特定のハードウェア用に最適化することが可能です。`compile_model()` 関数を呼ぶ際に、ターゲットとなるインスタンスファミリー (ここでは M4) とコンパイル済みのモデルを保存する S3 バケットを指定します。\n",
    "\n",
    "** [重要] もし以下のコマンドが permission error になる場合、ノートブックの上部にスクロールして `get_execution_role()` により返される execution role の値を確認して下さい。このロールには ``output_path`` で指定される S3 バケットへのアクセス権限が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/'.join(mnist_estimator.output_path.split('/')[:-1])\n",
    "optimized_estimator = mnist_estimator.compile_model(target_instance_family='ml_m4', \n",
    "                              input_shape={'data':[1, 784]},  # Batch size 1, 3 channels, 224x224 Images.\n",
    "                              output_path=output_path,\n",
    "                              framework='tensorflow', framework_version='1.11.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 SageMAker Neo でコンパイルしたモデルのデプロイ \n",
    "\n",
    "SageMaker Neo によってコンパイルされたモデルを推論インスタンスへデプロイします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_predictor = optimized_estimator.deploy(initial_instance_count = 1,\n",
    "                                                 instance_type = 'ml.m4.xlarge',\n",
    "                                                 wait=False)\n",
    "\n",
    "def numpy_bytes_serializer(data):\n",
    "    f = io.BytesIO()\n",
    "    np.save(f, data)\n",
    "    f.seek(0)\n",
    "    return f.read()\n",
    "\n",
    "optimized_predictor.content_type = 'application/vnd+python.numpy+binary'\n",
    "optimized_predictor.serializer = numpy_bytes_serializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.3 SageMaker Neo でコンパイルしたモデルでのリアルタイム推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    data = mnist.test.images[i]\n",
    "    \n",
    "    # Invoke endpoint with image\n",
    "    start = time.time()\n",
    "    predict_response = optimized_predictor.predict(data)\n",
    "    process_time = time.time() - start\n",
    "    process_times.append(process_time)\n",
    "        \n",
    "    label = np.argmax(mnist.test.labels[i])\n",
    "    prediction = np.array(predict_response).argmax()\n",
    "    print(\"label is {}, prediction is {}\".format(label, prediction))\n",
    "    \n",
    "print(\"推論実行時間の平均は {} です。\".format(np.array(process_times).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エンドポイントの削除\n",
    "\n",
    "このノートブックによって作られたリソースを削除していい場合、以下のセルを実行してください。このコマンドは上で作成したエンドポイントを削除して意図しない請求を防ぐことができます。\n",
    "(必要であれば、このノートブック自体を走らせているノートブックインスタンスも SageMaker のマネージメントコンソールから停止させて下さい。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(predictor_cpu.endpoint)\n",
    "sagemaker.Session().delete_endpoint(predictor_ei.endpoint)\n",
    "sagemaker.Session().delete_endpoint(optimized_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
